---
layout: page
title: "Performance Tuning"
alias: "performance-tuning"
category: "documentation/documentation"
date: 2011-04-23 20:08:18
redirect_from: /documentation/practical-x10-programming/performance-tuning.html
---
<div id="main-copy">
<p>This web page presents a quick summary of some key issues in performance tuning X10 applications with the goal of helping users understand and improve the performance of applications written in X10 2.4.&nbsp; Significantly more detail on this topic can be found in two papers presented at the X10'11 workshop at PLDI.&nbsp; We strongly recommend that you at least skim these papers if you are interested in performance tuning X10 applications.&nbsp;</p>
<ul>
<li><a href="http://x10.sourceforge.net/documentation/papers/X10Workshop2011/X10PerformanceModel.pdf">A Performance Model for X10 Applications</a> presents a general discussion of the X10 performance model and details of the Native X10 implementation (X10 compiled to C++)</li>
<li><a href="http://x10.sourceforge.net/documentation/papers/X10Workshop2011/CompilingX10ToJava.pdf">Compiling X10 to Java </a>contains an in depth discussion of Managed X10 performance (X10 compiled to Java).</li>
</ul>
<p>The rest of this page covers the following topics:</p>
<div>
<ul>
<li><a href="#PerformanceTuninganX10Application-BestPracticesforPerformanceTuning">Best Practices for Performance Tuning</a></li>
<li><a href="#PerformanceTuninganX10Application-Compilerandbuildoptionstomaximizeperformance">Compiler and build options to maximize performance</a></li>
<li><a href="#PerformanceTuninganX10Application-ConfiguringTheRuntime">Configuring the X10 Runtime</a></li>
<li><a href="#PerformanceTuninganX10Application-SelectingtherightX10RTImplementation">Selecting the right X10RT Implementation</a></li>
<li><a href="#PerformanceTuninganX10Application-TipsandTricksforPerformanceAnalysisofX10programs">Tips and Tricks for Performance Analysis of <span>X10</span> programs</a></li>
<li><a href="#PerformanceTuninganX10Application-ImplementationLimitationsandOtherPitfalls">Implementation Limitations and Other Pitfalls</a></li>
</ul>
<div class="section_2">
<h2><a name="PerformanceTuninganX10Application-BestPracticesforPerformanceTuning"></a>Best Practices for Performance Tuning</h2>
<div class="section_4">
<h5><a name="PerformanceTuninganX10Application-Functionalityfirst,Performancesecond"></a>Functionality first, Performance second</h5>
<p>It's always advisable to first get the code working and thoroughly tested first, then worry about making it perform.</p>
</div>
<div class="section_4">
<h5><a name="PerformanceTuninganX10Application-Usetherightcompilerflags(andtherightcompiler)."></a>Use the right compiler flags (and the right compiler).</h5>
<p>Be sure to compile your program with -O. Compile with -O -NO_CHECKS for peak performance once you are sure that the code is correct. For most programs, you will get better performance with <span>x10c</span>++, not <span>x10c</span>.&nbsp; The main exception are programs that are heavily object-oriented and/or have a very high allocation rate.</p>
</div>
<div class="section_4">
<h5><a name="PerformanceTuninganX10Application-Worryaboutsingleplaceperformancefirst"></a>Worry about single-place performance first</h5>
<p>Our experience has been that programs that perform poorly in a single place also don't scale. So before worrying about scaling, first make sure the single-place version of the code is reasonably efficient.</p>
</div>
<div class="section_4">
<h5><a name="PerformanceTuninganX10Application-Getgoodscalingforasmallnumberofplacesbeforeattemptinglargerruns"></a>Get good scaling for a small number of places before attempting larger runs</h5>
<p>Quite a bit of scaling performance work can be done with only a few <span>X10</span> places. In particular, doing runs with 2, 4, and 8 places is usually sufficient to identify non-scalable communication patterns, excessive data transfer, and excessive creation of remote-references to heap allocated objects (which will become memory leaks due to the lack of distributed GC).</p>
</div>
<div class="section_4">
<h5><a name="PerformanceTuninganX10Application-Scalingtoaverylargenumberofplaces(hundredsorthousands)requirescarefulusageofX10languagefeatures."></a>Scaling to a very large number of places&nbsp;</h5>
<p>Scaling to hundreds or thousands of places requires more careful usage of <span>X10</span> language features and more careful programming than scaling to a smaller number of places. For some examples of benchmarks that do scale, see the <span>X10</span> version of the <span>HPCC</span> benchmarks and the <span>UTS</span> benchmark.&nbsp; They can be found in <span>svn</span> at <a href="https://svn.code.sourceforge.net/p/x10/code/benchmarks/trunk/PERCS/">https://svn.code.sourceforge.net/p/x10/code/benchmarks/trunk/PERCS/</a> or as an optional download with each <span>X10</span> release. These codes have been successfully run on large <span>PowerPC</span> clusters and on <span>BlueGene</span> systems. Notice that they very carefully avoid using many <span>X10</span> language features, do not create very many remote references, and do not uses clocks, whens, <span>atomics</span>, or futures. We are continually updating these codes to be "as nice as possible" within the current limitations of the <span>X10</span> implementation, so spending some time understanding how this code works and why it is written the way it is will be very helpful if you want to try to scale other codes to large systems. As the <span>X10</span> implementation matures, we expect it to become progressively easier to scale programs, but it currently requires care and some amount of persistence.</p>
<p>As a rule of thumb, so far most <span>X10</span> programs that have scaled well to a large number of nodes have had the following control pattern:</p>
<ul>
<li>The main activity has a top-level finish under which it uses <span>asyncs</span> to create a top-level <span>async</span> at each place that runs for a long time. That top-level <span>async</span> may in turn use <span>asyncs</span>/at operations fairly freely to communicate asynchronously with the other places.&nbsp;</li>
<li>Bulk data transfers of primitive (non-pointer containing) data should utilize the asyncCopy methods of Rail and Array.</li>
<li>It may be possible to have nested local finishes, but large numbers of nested distributed finishes probably need to be avoided due to limitations in the current distributed finish implementation.</li>
</ul>
<p>One way to describe this control pattern as <span>SPMD</span> augmented with active messages.<br /> Of course, one may be able to use more general control patterns on subsets of nodes within a larger computation. We have not deeply explored all of the possible combinations.</p>
<p>We are always interested in more sample <span>X10</span> programs, especially ones that have been scaled up to run on large systems. If you have one, please consider contributing it back to the project!</p>
</div>
</div>
</div>
<div class="section_2">
<h2><a name="PerformanceTuninganX10Application-Compilerandbuildoptionstomaximizeperformance"></a>Compiler and build options to maximize performance</h2>
<p><span style="color: red;"><strong>The default compiler/build options are set to minimize compile time, not maximize performance!</strong></span></p>
<p>To do performance evaluation of X10, you need to be sure to use the right set of flags. Using the default flags will result in significantly lower than peak performance because (as is standard with C++ compilers), the default set of options result in compiling with no optimizations.</p>
<p>There are two options you need to pass to either the x10c++ or x10c compiler for peak performance</p>
<div class="table-wrap">
<table class="confluenceTable">
<tbody>
<tr><th class="confluenceTh">Option</th><th class="confluenceTh">Semantics</th></tr>
<tr>
<td class="confluenceTd">-O</td>
<td class="confluenceTd">enable optimization</td>
</tr>
<tr>
<td class="confluenceTd">-NO_CHECKS</td>
<td class="confluenceTd">disable array bounds checking, null pointer checking, and place checking</td>
</tr>
</tbody>
</table>
</div>
<p>Depending on the kind of code you are running, -NO_CHECKS may or may not have significant performance impact. For array based code where the arrays have rank&gt;1, -NO_CHECKS is currently critical for maximizing performance as the multi-dimensional&nbsp; array bounds checking code has not been designed for high performance.</p>
<p>You will also want to make sure that the X10 class libraries are compiled with the proper options as well. If you are using a pre-built binary release of X10, the standard library was compiled with -O, but not with -NO_CHECKS. For absolutely peak performance, you would need to rebuild the class libraries with -NO_CHECKS, however since the largest impact of -NO_CHECKS is for arrays and the basic array functions will be inlined into the application code with -O you may be able to simply compile the application code with -O -NO_CHECKS with minimal performance loss vs. recompiling the standard libraries as well.</p>
<p>Here is how to build the X10 standard libraries from source for absolute peak performance.</p>
<div class="code panel" style="border-width: 1px;">
<div class="codeContent panelContent">
<pre class="code-java">cd x10.dist
ant distclean; ant dist -Doptimize=<span class="code-keyword">true</span> -DNO_CHECKS=<span class="code-keyword">true</span>
</pre>
</div>
</div>
<p>And invoke x10c++ to compile your application like:</p>
<div class="code panel" style="border-width: 1px;">
<div class="codeContent panelContent">
<pre class="code-java">x10c++ -O -NO_CHECKS &lt;....<span class="code-keyword">rest</span> of command line...&gt;</pre>
</div>
</div>
</div>
<div class="section_2">
<h2><a name="PerformanceTuninganX10Application-ConfiguringTheRuntime"></a>Properly configuring the X10 Runtime</h2>
<p>The X10 runtime internally executes asyncs by scheduling them on a pool of worker threads within each place.&nbsp; By default, the X10 runtime only creates a single worker thread for place.&nbsp; To exploit multiple cores within a place, you must set the X10_NTHREADS environment variable to the desired number of worker threads to properly exploit the additional cores.&nbsp; A good rule of thumb is to create one X10 worker thread per available core.&nbsp; For example, suppose you wanted to run an X10 program with two places on a machine with 8 cores and wanted the program to use all the available cores.&nbsp; Then you should set X10_NTHREADS=4 (2 places x 4 threads per place = 8 active cores).&nbsp; The X10 runtime will endeavor to keep the number of active worker threads in its pool at the requested value by dynamically adding/removing threads as needed.&nbsp; For more details and related issues please see the Runtime section of the Performance Model paper.</p>
<h2><a name="PerformanceTuninganX10Application-SelectingtherightX10RTImplementation"></a>Selecting the right <span>X10RT</span> Implementation</h2>
<p>The sockets implementation of X10RT is supported on all platforms, but multi-place programs using it may not perform as well as alternative transports (higher latency, lower bandwidth). If it is available for your platform, use pami instead of sockets. As a second choice, use the MPI-based implementation of X10RT.</p>
<p>For more details, see <a href="/documentation/practical-x10-programming/x10rt-implementations.html" title="X10RT Implementations">X10RT Implementations</a></p>
</div>
</div>
<div class="section_2">
<div class="section_4">
<div class="section_2">
<h2><a name="PerformanceTuninganX10Application-TipsandTricksforPerformanceAnalysisofX10programs"></a>Tips and Tricks for Performance Analysis of <span>X10</span> programs</h2>
<div class="section_4">
<h5><a name="PerformanceTuninganX10Application-Identifyingsequentialbottlenecks:usestandardtoolingforC/C"></a>Identifying sequential bottlenecks: use standard tooling for C/C++</h5>
<p>Many profiling tools for C/C++ executables will "just work" with the executables produced by <span>x10c</span>++.</p>
<p>We have had decent luck with using <a href="https://code.google.com/p/gperftools/" target="_blank">g<span>perftools</span></a> to find bottlenecks in <span>X10</span> programs.&nbsp; First install <span>gperftools</span> on your machine. You can then give the <span>x10c</span>++ script the command line argument <span>-gpt</span> to cause it to link you <span>X10</span> executable with <span>libprofiler</span>. You then set the <span>CPUPROFILE</span> environment variable to an output file and run <span>pprof</span> to process the results (as documented in the <a href="http://gperftools.googlecode.com/svn/trunk/doc/cpuprofile.html"><span>perftool</span> wiki</a>).</p>
<p><span><a href="http://valgrind.org/">Valgrind</a></span> tools have also been used successfully, although it is typically necessary to build <span>X10</span> with GC disabled (<code>ant <span>-DDISABLE_GC</span>=true</code>).</p>
</div>
<div class="section_4">
<h5><a name="PerformanceTuninganX10Application-Manymultiplacebottleneckscanbeidentifiedbyenablingtracingoptions"></a>Many multi-place bottlenecks can be identified by enabling tracing options</h5>
<p>The C++ native runtime (<span>x10aux</span>) layer includes several tracing options that can help you examine multi-place performance. These tracing options are always enabled:</p>
<ul>
<li><span><span>X10RT_RXTX </span></span>Show the total bytes transmitted/received and the number of messages transmitted/received by each place. This information is printed at each place upon program termination.&nbsp; Some common experiments to try to see if you may have a serialization related performance problem are:
<ul>
<li>Run your program on two places and at several input sizes.&nbsp; Plot the bytes/messages sent as a function of input size.&nbsp; If there is a super-linear growth, then you are very likely to have problems scaling the program to larger inputs.&nbsp; Also consider if the numbers match your rough estimates of how much data should be moving between places. Note that there will be some nominal messaging/serialization related to initialing the X10 runtime that will be happen for all programs.&nbsp; For example, in X10 2.4 enabling X10RT_TX on the empty program and running it on two places yields:</li>
</ul>
</li>
</ul>
<p style="margin-left: 120px;">Place: 0&nbsp;&nbsp; msg_rx: 32/4&nbsp;&nbsp; msg_tx: 32/5<br />Place: 0&nbsp;&nbsp; put_rx: 0(&amp;0)/0&nbsp;&nbsp; put_tx: 0(&amp;0)/0<br />Place: 0&nbsp;&nbsp; get_rx: 0(&amp;0)/0&nbsp;&nbsp; get_tx: 0(&amp;0)/0<br />Place: 1&nbsp;&nbsp; msg_rx: 40/6&nbsp;&nbsp; msg_tx: 40/5<br />Place: 1&nbsp;&nbsp; put_rx: 0(&amp;0)/0&nbsp;&nbsp; put_tx: 0(&amp;0)/0<br />Place: 1&nbsp;&nbsp; get_rx: 0(&amp;0)/0&nbsp;&nbsp; get_tx: 0(&amp;0)/0</p>
<p style="margin-left: 80px;">The first line indicates that Place 0 received 4 messages totaling 32 bytes and sent 5 messages totaling 32 bytes.&nbsp; Place 1 received 6 messages totaling 40 bytes and sent 5 messages totaling 40 bytes.&nbsp; The numbers don't exactly match because some additional messages are sent to coordinate the printing of the message counts.</p>
<ul style="margin-left: 40px;">
<li>Run your program on the same input at 2, 4, and 8 places.&nbsp; Plot the bytes/messages/bytes sent as a function of the number of places.&nbsp; Also plot <span>messages/bytes</span> sent divided by the number of places as a function of the number of places.&nbsp; A clear danger sign is if serialization costs are increasing faster than the number of places.&nbsp; It may be ok for the serialization to grow linearly with the number of places, but it is usually not desirable.&nbsp; Programs that are likely to scale well will almost always will show decreasing bytes/place when run on a constant input with an increasing number of places.</li>
</ul>
<p>Some additional tracing options are available when the <span>X10</span> runtime is built without optimization (and are therefore not available in pre-built <span>X10</span> releases). All of the tracing options are enabled by setting environment variables. Some of the more useful additional tracing options are:</p>
<ul>
<li><span>X10_TRACE_SER</span> Trace serialization (data sent between places by <tt>at</tt> operations). It will also report on all global references that are actually serialized to another place (thus causing their values to become uncollectable by the garbage collector, representing a space leak).<br /> The tracing can perturb performance, but can be useful for identifying communication bottlenecks and memory leaks due to serialization.</li>
<li><span><span>X10_TRACE_X10RT</span></span> Trace calls into <span>X10RT</span></li>
<li><span>X10_TRACE_NET</span> Shorthand for <span>X10_TRACE_SER</span> and <span>X10_TRACE_X10RT</span></li>
</ul>
<p>All of these additional tracing options will print large amounts of output to <span><span>stdout</span></span>. The output will be prefixed by the place id. You will often want to redirect the output into a log file.&nbsp; A few typical ways to use these detailed logs are:</p>
<ul>
<li>Inspect serialization activity for specific X10 types or source level ats to identify unexpected field captures. This is unfortunately fairly tedious.&nbsp; You will often find the problem more easily by examining the source code of the program, injecting counters to discover which ats are executing frequently, and then carefully examining the source code of those ats to determine what is being serialized by their execution.</li>
<li>Extract the messages related to GlobalRefs being serialized to identify excessive cross-place pointer creation and memory retention.&nbsp; Do this by grepping for&nbsp; RefLogger: in the logfile.</li>
</ul>
<p style="margin-left: 40px;">[%1] ]$ grep RefLogger log.txt<br /> 0: SS: RefLogger: recording 0x20c99b0 as a new globally escaped reference<br /> 0: SS: RefLogger: 0x20c99b0 was already recorded as a globally escaped reference<br /> 0: SS: RefLogger: 0x20c99b0 was already recorded as a globally escaped reference<br /> 0: SS: RefLogger: recording 0x20c9820 as a new globally escaped reference<br /> 0: SS: RefLogger: 0x20c9820 was already recorded as a globally escaped reference<br /> 0: SS: RefLogger: 0x20c9820 was already recorded as a globally escaped reference<br /> 0: SS: RefLogger: recording 0x20c9500 as a new globally escaped reference<br /> 0: SS: RefLogger: 0x20c9500 was already recorded as a globally escaped reference<br /> 0: SS: RefLogger: 0x20c9500 was already recorded as a globally escaped reference<br /> &nbsp;</p>
</div>
</div>
</div>
</div>
<div class="section_2">
<h2><a name="PerformanceTuninganX10Application-ImplementationLimitationsandOtherPitfalls"></a>Implementation Limitations and Other Pitfalls</h2>
<p>Several language features are not implemented as well as we would like in X10 2.4. There are also weaknesses in the compiler's ability to optimize common idioms that can significantly impact performance. Here are some of the most common issues to be aware of.</p>
<div class="section_4">
<h6><a name="PerformanceTuninganX10Application-Languagefeaturestoavoidusinginperformancecriticalcode(oratall)."></a>Language features to avoid using in performance critical code</h6>
<ul>
<li>Don't use <tt>atomic</tt> in frequently executed code if you intend to run with multiple worker threads per place. Atomic blocks are implemented via a place-wide lock and therefore limit intra-place scalability.</li>
<li>Don't use <tt>when</tt> at all. In addition to the scalability problems of <tt>atomic</tt>, <tt>when</tt> usually entails additional blocking operations and often results in excessive thread usage by the fork-join runtime.</li>
<li>The performance of multi-dimensional general arrays (x10.regionarray.Array)&nbsp; is significantly reduced by array bounds checking code even with -O. You need to compile with -NO_CHECKS to get acceptable multi-dimensional array performance.</li>
<li>The default distributed finish implementation does not scale beyond a moderate number of places (hundreds). The issue is that finish termination messages are all routed to the "root node" of the finish, and the resulting message traffic can overwhelm the node.&nbsp; This can be overcome by using pragmas to select specialized finish implementations that do scale to tens of thousands of places.</li>
</ul>
<h6><a name="PerformanceTuninganX10Application-Languagefeaturestoavoidusinginperformancecriticalcode(oratall).2"></a>Be aware of async granularity.</h6>
<p>As discussed in the Performance Model paper, the default runtime uses a fork-join based implementation and co-operative scheduling.&nbsp; This has a number of performance implications. Some of the most important ones to keep in mind are:</p>
<ul>
<li>Large numbers of fine-grained asyncs are likely to hurt performance.&nbsp; In the fork-join based runtime, each async created results in somewhere around 50 or 100 machine instructions of overhead (allocation of a task object, pushing the task object onto a deque, and so forth).&nbsp; Therefore it is advisable that the bulk of the asyncs in your program represent non-trivial pieces of computation (minimal hundreds of machine instructions, ideally thousands of instructions).&nbsp; If a large number of small asyncs are created, the program will spend a large fraction of total execution cycles on activity creation related bookkeeping.</li>
<li>However, in multi-place programs overly coarse asyncs can also hurt overall performance.&nbsp; In particular, the runtime will only automatically poll the network and respond to incoming messages (at's) in between executing asyncs.&nbsp; If all of the workers in a place are busy executing long-running asyncs, then there may be long delays in executing "short" at's from other places, resulting in those remote places being idle and poor load balancing.&nbsp; A work around for this problem is to inject explicit calls to x10.lang.Runtime.poll() in the bodies of long-running asyncs to provide additional opportunities for the scheduler to process incoming messages from other places.</li>
<li>In general, an at statement or expression will cause the suspension of the current thread until the execution of the at completes at the remote place. This can result in a large number of worker threads being created (to replace the threads suspended by the at).&nbsp; However,&nbsp; the compiler/runtime optimize the special case of at (p) async S and implement it by immediately executing the sending portion of the implied message send on the current activity.&nbsp; So, when possible, consider using at (p) async S instead of just at (p) S.</li>
</ul>
<p>There is an experimental Cilk-style workstealing scheduler available in recent X10 releases (use x10c++ -WORK_STEALING=true to compile your program), however it is still in development and may have implementation limitations that prevent it from working on some input programs.</p>
<div class="section_4">
<h6><a name="PerformanceTuninganX10Application-Capturingof&quot;this&quot;andotherexcessiveserializationofglobalfields"></a>Capturing of "this" and other excessive serialization of objects</h6>
<p><span>X10</span> makes it easy to capture object large object graphs within an at body and serialize them to another place. Be aware that within an instance method it is easy to capture <tt>this</tt> in an at or closure without realizing it (it will be captured by accessing a field or calling an instance method). Part of tuning an application may entail manually rewriting code to grab just the parts of an object that the at actually needs, storing them into local variables, and then using the local variables in the at body.&nbsp; In some cases, you may also want to make some instance fields of a class transient to avoid them being serialized when their containing object is serialized.</p>
<h6><a name="PerformanceTuninganX10Application-serialization of large object graphs"></a>Serialization of very large object graphs</h6>
<p>When an object graph is captured by an <span style="font-family: courier new,courier,monospace;">at</span>, X10 serialization semantics require that a new isomorphic object graph be created at the destination place.&nbsp; This requires that the serialization code do enough bookkeeping of class instances to detect repeated references to the same object and faithfully recreate the shared structure at the destination.&nbsp; Internally, this is implemented by tracking objects that are being serialized in a hash table.&nbsp; For very large object graphs (tens or hundreds of thousands of objects) this can become a significant overhead.</p>
<p>A minor corollary to the semantics of object serialization, is that it is more efficient to serialize instances of structs than it is to serialize instances of classes.&nbsp; Therefore, there may be some additional performance benefit to choosing to represent&nbsp; aggregate data as a struct where possible (the data is immutable and subclassing is not needed).</p>
</div>
</div>
<div class="section_4">
<h6><a name="PerformanceTuninganX10Application-Avoidspecifyingexplicittypesforlocal{{val}}variables"></a>Avoid specifying explicit types for local <tt>val</tt> variables</h6>
<p>Type inference of local <tt>val</tt> declarations based on the initialization expression will often infer a more precise type than a human is willing to declare. Sometimes the precise type is critical to enable optimization. Therefore in X10, one should either avoid declaring types on local variables entirely or use the <tt>val x&lt;:T = ...</tt> form where the static type is taken as an upper bound, not a lower bound.</p>
</div>
<div class="section_4">
<h6><a name="PerformanceTuninganX10Application-Forloopoptimizationisoverlysensitivetotrivialdetailsofhowtheloopiswritten"></a>For loop optimization over x10.regionarray.Region is overly sensitive to trivial details of how the loop is written</h6>
<p>The compiler does a good job of converting some for loops over rectangular regions into counted for loop nests:</p>
<div class="code panel" style="border-width: 1px;">
<div class="codeHeader panelHeader" style="border-bottom-width: 1px;"><strong>for loop over a region using "un-named exploded form". Will be optimized</strong></div>
<div class="codeContent panelContent">
<pre class="code-java">val r = Region.make(1..10, 1..10);
<span class="code-keyword">for</span> ([i,j] in r) {
    ...compute based on i and j
}
</pre>
</div>
</div>
<div class="code panel" style="border-width: 1px;">
<div class="codeHeader panelHeader" style="border-bottom-width: 1px;"><strong>for loop over an array using "un-named exploded form". Will be optimized</strong></div>
<div class="codeContent panelContent">
<pre class="code-java">val r = Region.make(1..10, 1..10);
val a = <span class="code-keyword">new</span> Array[<span class="code-object">double</span>](r, ....)
<span class="code-keyword">for</span> ([i,j] in a) {
   ... a(i,j)...
}
</pre>
</div>
</div>
<p>However, very minor variations on the above loops will not perform as well due to weaknesses in the for loop expansion in the current compiler.&nbsp;</p>
<div class="code panel" style="border-width: 1px;">
<div class="codeHeader panelHeader" style="border-bottom-width: 1px;"><strong>for loop where Region has imprecise explicit type. NOT OPTIMIZED because doesn't know that the Region has the rect property</strong></div>
<div class="codeContent panelContent">
<pre class="code-java">val r:Region(2) = Region.make(1..10, 1..10);
<span class="code-keyword">for</span> ([i,j] in r) {
    ...compute based on i and j
}
</pre>
</div>
</div>
<div class="code panel" style="border-width: 1px;">
<div class="codeHeader panelHeader" style="border-bottom-width: 1px;"><strong>for loop using "named exploded form". NOT OPTIMIZED because the point is explicitly named even if p is never used in the loop!</strong></div>
<div class="codeContent panelContent">
<pre class="code-java">val r = Region.make(1..10, 1..10);
val a = <span class="code-keyword">new</span> Array[<span class="code-object">double</span>](r, ....)
<span class="code-keyword">for</span> (p[i,j] in a) {
   ... a(i,j)...
}
</pre>
</div>
</div>
<div class="code panel" style="border-width: 1px;">
<div class="codeHeader panelHeader" style="border-bottom-width: 1px;"><strong>for loop using points. NOT OPTIMIZED because the point is used as a point.</strong></div>
<div class="codeContent panelContent">
<pre class="code-java">val r = Region.make(1..10, 1..10);
val a = <span class="code-keyword">new</span> Array[<span class="code-object">double</span>](r, ....)
<span class="code-keyword">for</span> (p in a) {
   ... a(p)...
}

</pre>
</div>
</div>
</div>
</div>
